import tensorflow as tf

'''
# CUDA を有効にしている時に「UnknownError: Fail to find the dnn implementation」というエラーが
# 出て動かない時はコメントアウトを外し、カーネルを再起動してから実行してください
from tensorflow.compat.v1.keras.backend import set_session
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
config.log_device_placement = True
sess = tf.compat.v1.Session(config=config)
set_session(sess)
'''

# (注) numpy 1.20 以降を使うとエラーが出るようです(2022年1月時点)
# エラーが出る場合は 1.19 にダウングレードしてください
# (anacondaを使っている場合) conda install numpy=1.19
import numpy as np

import wave


# パーセプトロン/LSTMブロックの数
N = ?    # 入力層
K = ?    # 隠れ層(LSTMブロック)
M = ?    # 出力層

# 学習率
r = ?

# エポック数
E = ?

# バッチサイズ
B = ?

# Waveファイル読み込み関数
def read_wavedata(name):
    print('read: '+name)
    with wave.open(name,'r') as wavein:
        print(wavein.getparams())
        rate = wavein.getframerate()
        nframes = wavein.getnframes()
        data = wavein.readframes(wavein.getnframes())
    return np.frombuffer(data, dtype=np.int16),rate,nframes

# Waveファイルを読み込んで学習データセットを作成
classnames=['a','i','u','e','o']
wavedata_training=[]
classno_training=[]
wavedata_test=[]
classno_test=[]
for i in range(len(classnames)):
    x,rate,frames = read_wavedata(classnames[i]+'.wav') 
    # 500 msec 時点から 5 msec おきに 10 msec だけ信号を100個取り出して学習データとする
    for j in range(100):       
        start_pos = int( (500+5*j)*rate/1000 )
        end_pos = start_pos + int( 10*rate/1000)
        wavedata_training.append(x[start_pos:end_pos] )
        classno_training.append(i)
    # 502.5 msec 時点から 5 msec おきに 10 msec だけ信号を20個取り出してテストデータとする
    for j in range(20):
        start_pos = int( (502.5+5*j)*rate/1000 )
        end_pos = start_pos + int( 10*rate/1000 )
        wavedata_test.append(x[start_pos:end_pos] )
        classno_test.append(i)

# 信号長(サンプリング周波数によって変わる)
S = len(wavedata_training[0])

# 学習データセットのサイズ
L = len(wavedata_training)

# 学習データセットのサイズ
L_test = len(wavedata_test)

# 学習データセットを演習用のフォーマットに変換
data_training  = tf.constant( np.array(wavedata_training).reshape(L,S,N), dtype=tf.float32)
label_training = tf.constant( tf.keras.utils.to_categorical(classno_training, M), dtype=tf.float32)

# 今回は検証データセットは使わない

# テストデータセットを演習用のフォーマットに変換
data_test  = tf.constant( np.array(wavedata_test).reshape(L_test,S,N), dtype=tf.float32)
label_test = tf.constant( tf.keras.utils.to_categorical(classno_test, M), dtype=tf.float32)

print('')
print('N='+str(N))
print('K='+str(K))
print('M='+str(M))
print('信号長='+str(S))
print('学習率='+str(r))
print('エポック数='+str(E))
print('学習データセットサイズ='+str(L))
print('テストデータセットサイズ='+str(L_test))
print('バッチサイズ='+str(B))

# LSTMの構築
?

# 学習設定
loss = ?
opt = tf.keras.optimizers.Adam( learning_rate=r )
met = ?
model.compile(loss=loss, optimizer=opt, metrics=[met])

# モデルの概要表示
model.summary()

# 学習前の状態
ls, acc = model.evaluate(data_training, label_training, verbose=0)
print('\n学習前')
print('学習データセット　 : 損失={:0.2f}, 正解率={:0.2f}'.format(ls,acc))
print('')

# ディープラーニング実行
?

# 学習後の状態
ls, acc = model.evaluate(data_training, label_training, verbose=0)
ls_test, acc_test = model.evaluate(data_test, label_test, verbose=0)
print('\n\n学習後')
print('学習データセット　 : 損失={:0.2f}, 正解率={:0.2f}'.format(ls,acc))
print('テストデータセット : 損失={:0.2f}, 正解率={:0.2f}'.format(ls_test,acc_test))
print('クラス別の結果')
for i in range(len(classnames)):
    ls_test, acc_test = model.evaluate(data_test[20*i:20*i+20], label_test[20*i:20*i+20], verbose=0)
    print('{} : 損失={:0.2f}, 正解率={:0.2f}'.format(classnames[i],ls_test,acc_test))
