import tensorflow as tf

'''
# CUDA を有効にしている時に「UnknownError: Fail to find the dnn implementation」というエラーが
# 出て動かない時はコメントアウトを外し、カーネルを再起動してから実行してください
from tensorflow.compat.v1.keras.backend import set_session
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
config.log_device_placement = True
sess = tf.compat.v1.Session(config=config)
set_session(sess)
'''

# (注) numpy 1.20 以降を使うとエラーが出るようです(2022年1月時点)
# エラーが出る場合は 1.19 にダウングレードしてください
# (anacondaを使っている場合) conda install numpy=1.19
import numpy as np

import matplotlib.pyplot as plt

# パーセプトロン/LSTMブロックの数
N = ?    # 入力層
K = ?    # 隠れ層 (LSTMブロック)
M = ?    # 出力層

# 信号長
S = ?

# 学習率
r = ?

# エポック数
E = ?

# バッチサイズ
B = ?

# 元の信号(サイン波 + 一様乱数)
Td = 100  # サイン波の周期
i = np.arange(Td*2) # 2回振動させる
x = np.sin(2*np.pi/Td*i) + np.random.uniform(low=-0.1, high=0.1, size=Td*2)
print('元のサイン波')
plt.figure(figsize=(8, 8), dpi=50)
plt.ylim(-1.2,1.2)
plt.plot(range(0, len(x)), x)
plt.show()

# 元の信号から学習データセットを作成
# まず時刻 0 において x[0]〜x[S-1] を入力信号とし、x[S]をそのラベルとする
# 次に時刻 1 において x[1]〜x[S] を入力信号とし、x[S+1]をそのラベルとする
# これを繰り返してデータセットを作る
data_training=[]
label_training=[]
for i in range(len(x)-S):
    data_training.append(x[i:i+S])
    label_training.append(x[i+S])

# 学習データセットのサイズ(自動計算)
L = len(data_training)

# 学習データセットを演習用のフォーマットに変換
data_training  = np.array(data_training).reshape(L,S,N)
label_training = np.array(label_training).reshape(L,M)

# データ表示関数
def show_data(data,label,i):
    print('Data No.'+str(i))
    plt.figure(figsize=(1, 10), dpi=50)
    plt.xlim(-1,S+1)
    plt.ylim(-1.2,1.2)
    plt.plot(range(S),data[i])
    plt.plot(S,label[i],marker='.',markersize=20, color='red')
    plt.show()

print('データセットサイズ L = '+str(L))
print('例として No.0 と No.12 のデータを表示')
print('')
show_data(data_training,label_training,0)
show_data(data_training,label_training,12)

# 今回は検証データセットとテストデータセットは使わない

# LSTMの構築
?

# 学習設定
?

model.summary()

# 結果の表示関数
def show_result(x,predict):
    plt.figure(figsize=(8, 8), dpi=50)
    plt.ylim(-1.2,1.2)
    plt.plot(range(0, len(x)), x)
    plt.plot(range(S-1, len(predict)+S-1), predict, color="r")
    plt.show()
    
# 学習前の状態
print('\n学習前')
predict = model.predict(data_training)
show_result(x,predict)

# ディープラーニング実行
?

# 学習後の状態
print('\n\n学習後')
predict = model.predict(data_training)
show_result(x,predict)
