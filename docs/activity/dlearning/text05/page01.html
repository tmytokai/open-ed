<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="stylesheet" href="../../../common.css">
<script src="../../../common.js"></script>
<script src="../../../mathjax.js"></script>
<title>1. LSTMとは</title>
</head>
<body>

<nav class="brcr">
<ol>
<li><a href="../">アクティビティ: TensorFlow/Keras によるディープラーニング</a></li>
<li>学習項目: [5] Keras を用いた LSTM の学習</li>
<li><script>GetTitle()</script></li>
</ol>
</nav>

<h2><script>GetTitle()</script></h2>


<p>
<a href="https://ja.wikipedia.org/wiki/%E9%95%B7%E3%83%BB%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6">LSTM (Long Short Term Memory: 長・短期記憶)</a>とは<a href="https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF">RNN (Recurrent Neural Network: 回帰型ニューラルネットワーク)</a>の一種ですので、最初に RNN について説明します。
<br>
なお今回は話を簡単にするために 3 層 RNN だけを考えます。
</p>

<br>
<h3>1. RNN (Recurrent Neural Network) : 回帰型ニューラルネットワーク</h3>

<p>
まずニューラルネットワークの入力層への入力信号として、以下の様な N 個の長さ S のシーケンス、つまり<a href="../../d-signal/text01/page02.html">時間領域ディジタル信号</a>を考えます。
</p>
<p>
※ これまで学んできた<a href="../text02/page02.html">MLP</a>や<a href="../text04/page01.html">CNN</a>には時間の概念が無いことに注意して下さい
</p>

<br>
\[
\left \{
\begin{align*}
& x_0[i] , (i=0,1,\cdots,\textrm{S}-1) \\
& x_1[i] , (i=0,2,\cdots,\textrm{S}-1) \\
& \cdots \\
& x_{(\textrm{N}-1)}[i], (i=0,1,\cdots,\textrm{S}-1)
\end{align*}
\right .
\]

<br>
<p>
ところで<a href="../../acr/index.html">自己相関関数</a>のアクティビティで学んだように、ある時刻における信号値はそれ以前の信号値と強い相関を持っていることが多いです。
<br>
よってニューラルネットワークに入力された信号値をネットワーク内でフィードバックして再利用することでネットワークのパラメータ数を減らすことが出来るはずです。
<br>
その様な考えのもとで開発されたのが RNN です。
</p>

<p>
では基本的な RNN のネットワーク構成図を図 1 に示します。
<br>
この図の様に RNN は<a href="../text02/page02.html">MLP</a>の隠れ層の出力を入力側にフィードバックさせている構成となっています。
<br>
つまり各時刻 $i$ において N 個の入力 $x_0[i], \cdots, x_{(\textrm{N}-1)}[i]$、 および隠れ層からの K 個の フィードバック入力 $h_0[i-1], \cdots, h_{(\textrm{k}-1)}[i-1]$ が隠れ層に入力され、出力層から M 個の出力信号 $y_0[i] \cdots y_{(\textrm{M}-1)}[i]$ が出力されます。
</p>



<div class="info">
<input type="checkbox"><b>図1. (3層) RNN の構成図</b>

<p>　</p>

<img src="./img/page01-fig1.png" alt="">

<p>
$z^{-1}$ ・・・ 1時刻遅延素子
</p>

</div>


<br>
<h3>2. LSTM(Long Short Term Memory) : 長・短期記憶</h3>

<p>
※ LSTMについては<a href="https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577">こちらのサイト</a>の説明が分かりやすいです
</p>

<p>
ところが RNN には隠れ層への入力($x_0[i], \cdots, x_{(\textrm{N}-1)}[i]$ および $h_0[i-1], \cdots, h_{(\textrm{k}-1)}[i-1]$)にかけられる重みの学習が不安定であるという問題があります。
</p>

<p>
そこで RNN を改良した LSTM が開発されました。
<br>
LSTM のネットワーク構成図を図 2 に示します。
<br>
RNN との違いは隠れ層のユニットとしてパーセプトロンの代わりに「<b>LSTMブロック</b>」を使っているところだけです。
</p>

<div class="info">
<input type="checkbox"><b>図2. (3層) LSTM の構成図</b>

<p>　</p>

<img src="./img/page01-fig2.png" alt="">

<p>
$z^{-1}$ ・・・ 1時刻遅延素子
</p>

</div>

<p>
この LSTM ブロックの内部は図 3 の様になっています。
<br>
なおこの図では分かりやすいように1入力・1フィードバック入力としていますが、多入力・多フィードバック入力の場合も同様に考えられます。
</p>

<div class="info">
<input type="checkbox"><b>図3. LSTM ブロック(1入力・1フィードバック入力の場合)の内部</b>

<p>　</p>

<img src="./img/page01-fig3.png" alt="">

<p>
$z^{-1}$ ・・・ 1時刻遅延素子
</p>
<p>
$\sigma$ ・・・ <a href="https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0">シグモイド関数</a>
</p>
<p>
CEC ・・・ Constant Error Carousel : 定誤差カルーセル
</p>
<br>
<p>
$w_x$ ・・・ 入力部において入力信号に掛けられる重み
</p>
<p>
$w_h$ ・・・ 入力部においてフィードバック入力信号に掛けられる重み
</p>
<p>
$b$  ・・・ 入力部のバイアス
</p>

<br>
<p>
$w_x^{\textrm in}$ ・・・ 入力ゲートにおいて入力信号に掛けられる重み
</p>
<p>
$w_h^{\textrm in}$ ・・・ 入力ゲートにおいてフィードバック入力信号に掛けられる重み
</p>
<p>
$b^{\textrm in}$  ・・・ 入力ゲートのバイアス
</p>

<br>
<p>
$w_x^{\textrm fg}$ ・・・ 忘却ゲートにおいて入力信号に掛けられる重み
</p>
<p>
$w_h^{\textrm fg}$ ・・・ 忘却ゲートにおいてフィードバック入力信号に掛けられる重み
</p>
<p>
$b^{\textrm fg}$  ・・・ 忘却ゲートのバイアス
</p>

<br>
<p>
$w_x^{\textrm out}$ ・・・ 出力ゲートにおいて入力信号に掛けられる重み
</p>
<p>
$w_h^{\textrm out}$ ・・・ 出力ゲートにおいてフィードバック入力信号に掛けられる重み
</p>
<p>
$b^{\textrm out}$  ・・・ 出力ゲートのバイアス
</p>

</div>

<p>
LSTM の肝となるのが「<b>CEC</b>」と呼ばれる内部ユニットです。
</p>

<p>
RNNの学習が不安定な原因は「過去に入力した信号の情報がフィードバックを繰り返す(つまり何度もtanhに通される)ことによって徐々に消滅していくこと」なので、CECを導入することによって過去に入力した信号の情報をLSTMブロック内で長期的に記憶し、短期的な情報であるフィードバック入力と足し合わせることにしました。
</p>
<p>
※ これが「LSTM：長・短期記憶」の語源となっています
</p>

<p>
ただこのままだと入力信号の傾向が代わっても古い情報を CEC は記憶し続けてしまいます。
<br>
そこで「<b>忘却ゲート</b>」を導入して、もし入力信号の傾向が代わったら CEC をリセットすることにしました。
<br>
ついでに「<b>入力ゲート</b>」と「<b>出力ゲート</b>」も導入し、入力信号の傾向が代わったら入力や出力を行わないように改良を加えました。
</p>

<p>
この様に、LSTM は CEC と入力・忘却・出力の3つのゲートを導入することで安定して重みの学習を行えるようになっています。
</p>

<br>
<script>PreNext(1,3)</script>
</body>
</html>
