<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="stylesheet" href="../../../common.css">
<script src="../../../common.js"></script>
<script src="../../../mathjax.js"></script>
<title>4. ディープラーニングの実行</title>
</head>
<body>

<nav class="brcr">
<ol>
<li><a href="../">アクティビティ: TensorFlow によるディープラーニング</a></li>
<li>学習項目: [2] ディープラーニングの基本</li>
<li><script>GetTitle()</script></li>
</ol>
</nav>

<h2><script>GetTitle()</script></h2>

<br>
<h3>
1. ディープラーニングの概要
</h3>

<p>
<a href="./page03.html">前ページ</a>で 3 層ニューラルネットワークのデータフロー・グラフを作りましたが、隠れ層と出力層の重みやバイアス w_h、b_h、w_o、b_o は乱数で初期化していますので、このままでは入力信号 data を入力しても全く意味の無い出力信号 y_o が出力されます。
</p>
<p>
したがって、何らかの入力信号を与えた時に理想的な出力がされるように「<a href="https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0">ディープラーニング</a>」を使ってニューラルネットワークを学習(＝重みやバイアスを適切な値を変化させる)必要があります。
</p>

<p>
改めて書くと、ディープラーニングの定義は
</p>
<p>
「<b>教師信号が与えられた時にそれに対応する理想的な信号が出力されるようにニューラルネットワークの重みやバイアスなどのパラメータ値を更新する<a href="https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習(マシンラーニング)</a>の手法の一つ</b>」
</p>

<p>
です。
</p>

<p>
ここで「<b>教師信号</b>」とは文字通りニューラルネットワークを教育するための教師役となる信号のことで「<b>説明変数</b>」と呼ぶこともあります。
</p>
<p>
一方、ある教師信号を与えたときの理想的な出力信号のことを「<b>目的変数</b>」と呼びますが、特に「多クラス分類問題」を扱う場合は目的変数の事を「<b>ラベル</b>」と呼んでいます。
<br>
今回はこの「多クラス分類問題」を取り扱います。
</p>

<p>
なお教師信号を用いる学習のことを「<a href="https://ja.wikipedia.org/wiki/%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92">教師あり学習</a>」と呼びます。
<br>
逆に教師信号が無い機械学習の事を「<a href="https://ja.wikipedia.org/wiki/%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92">教師なし学習</a>」と言いますが今回は取り扱いません。
</p>


<br>
<h3>
2. 教師信号
</h3>

<p>
教師信号は<a href="./page03.html">前ページ</a>で考えた入力信号 data のように行列で表されます。
<br>
ただし学習を行うためには教師信号とラベルの組(データセット)を複数用意する必要があります。
</p>

<p>
そこで教師信号全体の名前を teacher とし、データセットが No.0 から No.(L-1) まで L 組あることにすると、今考えている 3 層ニューラルネットワークの入力層のパーセプトロンの個数は N 個でしたので、 teacher は LxN 行列
</p>

\[
{\rm teacher} = 
\begin{bmatrix}
t_{00} \ , & \cdots &,\  t_{\rm 0(N-1)} \\
\vdots & \ddots & \vdots \\
t_{\rm (L-1)0} ,  & \cdots &,\  t_{\rm (L-1)(N-1)} \\
\end{bmatrix}
\]

<p>
となります。
<br>
teacher の j 行目がデータセット No.j に相当します。
</p>

<p>
よって TensorFlow では教師信号は図 1 の様に L x N 行列の<a href="../text01/page04.html#const">定数テンソル</a>で定義されます。
</p>

<div class="info">
<input type="checkbox"><b>図1: 教師信号</b>

<p>
teacher : 教師信号、 L x N 行列 (<a href="../text01/page04.html#const">定数テンソル</a>)、j 行目がデータセット No.j に相当
</p>

<img src="./img/page04-fig1.png" alt="">
</div>

<br>
<h3>
3. ラベル
</h3>

<p>
次はラベルについて考えます。
</p>

<p>
今考えている 3 層ニューラルネットワークの出力層のパーセプトロンの個数は M 個で、データセットの組は No.0 から No.(L-1) までの L 個としましたので、 ラベル全体の名前を label とすると、label は LxM 行列
</p>

\[
{\rm label} = 
\begin{bmatrix}
l_{00}\ , & \cdots &,\  l_{\rm 0(M-1)} \\
\vdots & \ddots & \vdots \\
l_{\rm (L-1)0} \ , & \cdots &,\  l_{\rm (L-1)(M-1)} \\
\end{bmatrix}
\]

<p>
 で表すことができます。
</p>

<p>
さて、このラベルの値をどのように決めるかについては色々な形式があるのですが、今回は「<a href="https://ja.wikipedia.org/wiki/One-hot">one-hot ベクトル</a>形式」を使いたいと思います。
</p>

<p>
one-hot ベクトル形式とは、あるクラスに属する場合は 1、それ以外は 0 とする様にラベルを決める形式です。
</p>

<p>
例えばニューラルネットワークに入力された画像を「猫」と「犬」と「鳥」の 3 クラスに分類したい場合を考えてみましょう。
<br>
この場合は、M = 3 とし、データセット No.i における教師信号が猫(クラス No.0 とします)の画像だったら $l_{i0} = 1$ 、犬の画像(クラス No.1 とします)だったら $l_{i1} = 1$ 、鳥の画像(クラス No.2 とします)だったら $l_{i2} = 1$ 、それ以外は 0 の値を label にセットします。
<br>
つまり
</p>

<p>
猫ラベル(クラス No.0)・・・ $\{1,0,0\}$
<br>
犬ラベル(クラス No.1)・・・ $\{0,1,0\}$
<br>
鳥ラベル(クラス No.2)・・・ $\{0,0,1\}$
</p>

<p>
とします。
</p>

<p>
よって TensorFlow ではラベルは図 2 の様に L x M 行列の<a href="../text01/page04.html#const">定数テンソル</a>で定義されます。
</p>

<div class="info">
<input type="checkbox"><b>図2: ラベル</b>

<p>
label : ラベル、 L x M 行列 (<a href="../text01/page04.html#const">定数テンソル</a>)、j 行目がデータセット No.j に相当
</p>

<img src="./img/page04-fig2.png" alt="">

</div>

<br>
<h3>
4. 多クラス分類問題におけるニューラルネットワークの出力の意味
</h3>

<p>
上で挙げた多クラス分類問題を扱う時、ニューラルネットワークの出力はどういう意味を持つのでしょうか？
<br>
それを考えるため、まず教師信号 teacher をニューラルネットワークに入れて出てきた出力信号を LxM 行列
</p>

\[
{\rm predict} = 
\begin{bmatrix}
p_{00}\ , & \cdots &,\  p_{\rm 0(M-1)} \\
\vdots & \ddots & \vdots \\
p_{\rm (L-1)0}\ , & \cdots &,\  p_{\rm (L-1)(M-1)} \\
\end{bmatrix}
\]

<p>
とします。
</p>

<p>
また今回は出力層のパーセプトロンの活性化関数を<a href="../text01/page04.html#softmax">softmax 関数</a>としたため
</p>

<p>
\[
0 \leq p_{ij} \leq 1 \ ,\  \sum_{j=0}^{\rm M-1} p_{ij} = 1
\]
</p>

<p>
という関係が成り立っています。
</pr>

<p>
つまり、多クラス分類問題を考える場合、
</p>

<p>
<b>
「ニューラルネットワークの出力 $p_{ij}$ はデータセット No.i の教師信号がクラス No.j に属する予想確率」
</b>
</p>

<p>
を表します。
</p>

<br>
<h3>
5. 損失関数とカテゴリカル・クロスエントロピー
</h3>

<p>
教師信号とラベルのデータセットを用意したら、次は重みとバイアスをディープラーニングを使って学習・更新します。
</p>

<p>
ただし何らかの指標が無いときちんと学習されているか分かりませんので、まずその指標を決める必要があります。
<br>
この指標の事を「<b>損失関数</b>(loss function)」と呼びます。
<br>
この損失関数には色々な種類がありますが、今回は多クラス分類問題でよく使われている「<b>カテゴリカル・クロスエントロピー</b>(categorical cross entropy)」を利用したいと思います。
</p>

<p>
カテゴリカル・クロスエントロピーは上で定義したラベル(label)と予想確率(predict)それぞれの要素 $l_{ij}$ と $p_{ij}$ を使って次の様に定義されます。
</p>

<div class="info">
<input type="checkbox"><b>定義: カテゴリカル・クロスエントロピーの定義</b>

<p>
\[
{\rm entropy} = -\sum_{i=0}^{\rm L-1} \sum_{j=0}^{M-1} l_{ij}\log p_{ij}
\]
</p>

</div>

<p>
カテゴリカル・クロスエントロピーを計算した値 entropy は
</p>

<ol>
<li><b>ラベル(label)と予想確率(predict)が似ていない → entropy は大</b></li>
<li><b>ラベル(label)と予想確率(predict)が似ている → entropy は小</b></li>
</ol>

<p>
というとても良い性質を持っていますので、entropy が可能な限り小さくなる様に重みとバイアスの値を更新すれば良い事が分かります。
</p>

<p>
ところでカテゴリカル・クロスエントロピーは<a href="../text01/page04.html#redsum">総和演算</a> を使って次のような行列演算で求める事が出来ます。
</p>

<div class="info">
<input type="checkbox"><b>カテゴリカル・クロスエントロピーの行列演算</b>
<p>
entropy = - reduce_sum( log(predict)*label )
</p>
</div>

<p>
従ってカテゴリカル・クロスエントロピーの演算をデータフロー・グラフ化すると次のようになります。
</p>

<div class="info">
<input type="checkbox"><b>図3: カテゴリカル・クロスエントロピーのデータフロー・グラフ(演算部のみ抜粋)</b>

<p>
<br>
予測確率 predict に<a href="../text01/page04.html#log">log</a> を通し、ラベル label と<a href="../text01/page04.html#mul">掛け合わせる</a>、さらに<a href="../text01/page04.html#redsum">総和演算</a>をして -1 倍して entropy に出力する
</p>

<img src="./img/page04-fig3.png" alt="">
</div>

<p>
predict を求めている 3 層ニューラルネットワーク部分も含めると、全体では次のようなデータフロー・グラフとなります。
</p>

<div class="info">
<input type="checkbox"><b>図4: カテゴリカル・クロスエントロピーのデータフロー・グラフ(全体)</b>

<p>　</p>

<img src="./img/page04-fig4.png" alt="">
</div>



<br>
<h3>
6. 最適化アルゴリズム
</h3>

<p>
損失関数を用意したので、次は重みやバイアスの学習方法について考えます。
</p>

<p>
重みやバイアスを学習・更新するアルゴリズム(<b>最適化アルゴリズム</b>と呼びます)には色々あるのですが、今回は「<a href="https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95">SGD</a>(Stochastic Gradient Descent: 確率的勾配降下法)」と「<b>Adam</b>(ADAptive Moment estimation)」の 2 つを取り扱います。
</p>

<p>
SGD はベーシックな最適化アルゴリズムなのでチュートリアルなどでは良く使われてるアルゴリズムなのですが、学習の収束速度が遅いので、実際には Adam が使われることが多いようです。
</p>

<p>
さて、SGD や Adam を実行するためは非常に難しい数学の知識が必要なのですが、幸いなことに TensorFlow ではクラスとして既に用意されているので誰でも簡単に利用できます。
</p>

<div class="info">
<input type="checkbox"><b>SGD クラスと Adam クラス </b>

<p>
SGD クラス: tf.keras.optimizers.SGD( learning_rate=学習率 )
</p>

<p>
Adam クラス: tf.keras.optimizers.Adam( learning_rate=学習率 )
</p>

<p>
学習実行メソッド: minimize( lambda: 損失関数, [学習対象の変数のリスト] )・・・学習を1回実施する、戻り値は実施済み学習回数
</p>

</div>

<p>
ここで学習率は学習の精度と速度を表しています。
<br>
学習率の値が大きいほどニューラルネットワークは適当に学習しますが速く学習が進みます。
<br>
逆に値が小さいとニューラルネットワークはきちんと学習しますが遅く学習が進みます。
</p>

<p>
いずれにしろ、1 回では学習は終わりませんので、損失関数の値が十分小さくなるまで何回も学習を繰り返す必要があります。
</p>

<p>
例えば以下のソース 1 は Adam を用いた学習例です。
<br>
この例では学習対象である変数 x と y の初期値をそれぞれ 1,0 と-0.5、損失関数を ${\rm loss}() = x^2+y^2$、学習率を 0.1、学習回数を 50 回 としたとき、loss() が最小になる x と y の値(つまり (x,y)=(0,0)) を Adam を使って求めています。
</p>

<div class="info">
<input type="checkbox"><b>ソース 1: Adam による学習例</b>

<pre class="wrap">
import tensorflow as tf

# 学習対象の変数
x = tf.Variable([[1]], dtype=tf.float32)
y = tf.Variable([[-0.5]], dtype=tf.float32)

#損失関数
@tf.function
def loss():
    return x**2 + y**2

print('損失='+str(loss().numpy()))
print('x='+str(x.numpy()))
print('y='+str(y.numpy()))
print('')

opt = tf.keras.optimizers.Adam( learning_rate=0.1 ) # Adamクラスのインスタンス
for i in range(50):  # 学習を50回繰り返す
    opt.minimize(lambda: loss(), [x,y])
    
print('結果')
print('損失='+str(loss().numpy()))
print('x='+str(x.numpy()))
print('y='+str(y.numpy()))
print('')
</pre>
</div>

<p>
結果は以下のようになります。
<br>
学習を50回繰り返すと損失関数の値が十分小さくなり、 (x,y) の値が (0,0) に近づいていることが分かります。
</p>

<div class="info">
<input type="checkbox"><b>ソース 1 の結果</b>

<pre class="wrap">
損失=[[1.25]]
x=[[1.]]
y=[[-0.5]]

結果
損失=[[0.0007694]]
x=[[-0.00481954]]
y=[[-0.02731619]]
</pre>
</div>

<br>
<h3>
7. バッチサイズとエポック数
</h3>

<p>
実際のディープラーニングでは教師信号とラベルのデータセットの個数は膨大となるため、データセット全てを使って一気に学習を行うことは滅多にありません。
</p>

<p>
ではどうするかというと、データセットを更に「<b>バッチ(batch)</b>」というサブデータセットに細かく分割し、バッチ単位で学習を行います。
<br>
この学習方法のことを「<b>ミニバッチ学習</b>」といいます。
</p>

<p>
またバッチに含まれる教師信号とラベルの数を「<b>バッチサイズ</b>」と呼び、慣習的には 32,64,128,256,・・・・ など 2 の n 乗の数が良く使われています。
</p>

<p>
バッチの分割方法も色々あるのですが、今回は単純にデータセットの先頭から順に取り出すことにします。
<br>
つまりバッチサイズを B としたとき、バッチ No.i の教師信号 teacher_batch_i は
</p>

\[
{\rm teacher\_batch\_i} =
\begin{bmatrix}
t_{(B*i)0} \ , & \cdots &,\  t_{\rm (B*i)(N-1)} \\
\vdots & \ddots & \vdots \\
t_{\rm (B*i+B-1)0} ,  & \cdots &,\  t_{\rm (B*i+B-1)(N-1)} \\
\end{bmatrix}
\]


<p>
ラベル label_batch_i は
</p>

\[
{\rm label\_batch\_i} =
\begin{bmatrix}
l_{(B*i)0} \ , & \cdots &,\  l_{\rm (B*i)(N-1)} \\
\vdots & \ddots & \vdots \\
l_{\rm (B*i+B-1)0} ,  & \cdots &,\  l_{\rm (B*i+B-1)(N-1)} \\
\end{bmatrix}
\]

<p>
となります。
</p>

<p>
ところでデータセット全体のサイズは L 個でしたのでバッチは L/B 個あります。
<br>
バッチ No.0 から順にバッチ No.(L/B)-1 まで全てのバッチを使って一通り学習を行うことを「<b>エポック</b>(epoch)」と呼び、エポックの繰り返し回数のことを「<b>エポック数</b>」と呼びます。
</p>

<p>
用語だけ書いてもなかなか分かりにくいのですが、例えばエポック数が 3 のとき、次のようにして学習が行われます。
</p>

<p>
学習開始
<br>
→ (エポック 0) パッチNo.0を使って学習 → ・・・ → パッチNo.(L/B)-1 を使って学習 (エポック 0 終了) 
<br>
→ (エポック 1) パッチNo.0を使って学習 → ・・・ → パッチNo.(L/B)-1 を使って学習 (エポック 1 終了) 
<br>
→ (エポック 2) パッチNo.0を使って学習 → ・・・ → パッチNo.(L/B)-1 を使って学習 (エポック 2 終了) 
<br>
3回エポックを繰り返したので学習終了
</p>

<br>
<h3>
8. ディープラーニングの実行
</h3>

<p>
では全ての準備が整ったのでいよいよディープラーニングを実行してみましょう。
</p>

<p>
今回考えている 3 層ニューラルネットワークの場合、学習対象の変数は w_h、b_h、w_o、b_o の4つですので、結局のところ次のように書けばディープラーニングが実行されます。
</p>

<div class="info">
<input type="checkbox"><b>ソース 2: 3 層ニューラルネットワークのディープラーニング</b>

<pre class="wrap">
opt = tf.keras.optimizers.Adam( learning_rate=r )
for e in range(E):
    for i in range(int(L/B)):
        print('\repoch '+str(e)+' batch '+str(i), end=' ')
        teacher_batch_i = teacher[B*i:B*i+B]
        label_batch_i = label[B*i:B*i+B]
        opt.minimize(lambda: loss(teacher_batch_i,label_batch_i), [w_h,b_h,w_o,b_o])

※1 r : 学習率
※2 E : エポック数
※3 L : データセットサイズ
※4 B : バッチサイズ
※5 teacher, label : 教師信号とラベル全体、
※6 w_h, b_h, w_o, b_o : それぞれ隠れ層の重み、隠れ層のバイアス、出力層の重み、出力層のバイアス
※7 loss(教師信号,ラベル) : カテゴリカル・クロスエントロピーを計算している損失関数
</pre>
</div>



<br>
<script>PreNext(4,5)</script>
</body>
</html>
