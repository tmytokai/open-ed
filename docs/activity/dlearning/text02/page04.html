<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="stylesheet" href="../../../common.css">
<script src="../../../common.js"></script>
<script src="../../../mathjax.js"></script>
<title>4. ディープラーニングの実行</title>
</head>
<body>

<nav class="brcr">
<ol>
<li><a href="../">アクティビティ: TensorFlow によるディープラーニング</a></li>
<li>学習項目: [2] ディープラーニングの基本</li>
<li><script>GetTitle()</script></li>
</ol>
</nav>

<h2><script>GetTitle()</script></h2>

<br>
<h3>
1. ディープラーニングの概要
</h3>

<p>
前ページで(3 層)ニューラルネットワークのデータフローグラフの作り方を学びました。
<br>
ただし隠れ層と出力層の重み変数やバイアス変数はまだ乱数で初期化したままですので、今のままでは全く意味の無い出力信号がネットワークから出てきます。
<br>
そこでここではニューラルネットワークの「<b>機械学習</b>」(※)、通称「<b>ディープラーニング</b>」について簡単に説明したいと思います。
</p>

<p>
※ 動物の学習プロセスをソフトやハードで再現する技術一般のことを「機械学習」とか「マシンラーニング」と呼びます。
</p>

<p>
ディープラーニングとは
</p>
<p>
「<b>教師信号を与えた時に理想的な信号が出力されるようにニューラルネットワークの重みやバイアスの値を決めること</b>」
</p>

<p>
です。
<br>
ここで「<b>教師信号</b>」とは文字通りニューラルネットワークを教育するための教師役の信号のことです。
<br>
この教師信号は更に<b>入力側教師信号</b>と<b>出力側教師信号</b>の二通りあります。
<br>
なお教師信号が無い場合の機械学習もありますが今回は取り扱いません。
</p>

<p>
さてディープラーニングを行うと、入力側教師信号を学習中のニューラルネットワークに入れて出てきた出力信号と出力側教師信号がそっくりになるように重みとバイアスが更新されます。
<br>
ただし人間と同様に一回で学習するのは無理なので何千回も何万回も何億回も学習を繰り返す必要があります。
</p>

<p>
という訳で TensorFlow を使って高速学習を行うためには入力側教師信号と出力側教師信号も OP ノードで表す必要があります。
<br>
なお出力側教師信号は別名「<b>ラベル</b>」と呼ばれますので、特に断りの無い限り
</p>
<p>
<b>
入力側教師信号 → 「教師信号」
<br>
出力側教師信号 → 「ラベル」
</b>
</p>

<p>
と呼ぶことにします。
<br>
また教師信号とラベルの組は全部で L 組あるとします。
</p>

<br>
<h3>
2. 教師信号(入力側教師信号)
</h3>

<p>
まずニューラルネットワークへ入力される教師信号を OP ノードで表してみます。
<br>
この様な教師信号としては画像データのピクセル値とか音声データとか、データの種類によって色々な形式が考えられます。
</p>

<p>
今回取り扱っている 3 層ニューラルネットワークの入力信号は N 次元ベクトルとしたので教師信号は N 次元ベクトルが L 組あることになります。
<br>
つまり $i$ 番目の教師信号はベクトル $\{x_{i1}, x_{i2}, \cdots, x_{iN}\},(i = 1,2,\cdots,L)$ で表すことができるので、教師信号全体は図 1 の様に L x N 行列の<a href="../text01/page04.html#const">定数 OP ノード</a>で表す事が出来ます。
</p>

<div class="info">
<input type="checkbox"><b>図1: 教師信号:</b>

<p>
L x N 行列の教師信号を<a href="../text01/page04.html#const">定数 OP ノード</a>として表現する
<br>
$i$ 行目の $[x_{i1}, x_{i2}, \cdots, x_{iN}]$ が $i$ 番目の教師信号となる
</p>

<img src="./img/page04-fig1.png" alt="">
</div>

<br>
<h3>
3. ラベル(出力側教師信号)
</h3>

<p>
次はラベルを OP ノードで表してみます。
</p>

<p>
今回取り扱っている 3 層ニューラルネットワークの出力は M 次元ベクトルとしたのでラベルは M 次元ベクトルが L 組あることになります。
<br>
つまり $i$ 番目のラベルはベクトル $\{y_{i1}, y_{i2}, \cdots, y_{iM}\},(i = 1,2,\cdots,L)$ で表すことができます。
</p>

<p>
さて今回のアクティビティでは「入力信号の<b>分類問題</b>」を取り扱いたいと思います。
<br>
例えばニューラルネットワークに入力された画像を「猫」と「犬」と「鳥」の 3 クラス(M = 3)に分類したい場合を考えてみましょう。
<br>
この場合、$i$ 番目の教師信号が猫の画像だったら $y_{i1} = 1$ 、犬の画像だったら $y_{i2} = 1$ 、鳥の画像だったら $y_{i3} = 1$ ラベルを各画像に割り振ります。
<br>
つまり
</p>

<p>
猫ラベル・・・ $\{1,0,0\}$
<br>
犬ラベル・・・ $\{0,1,0\}$
<br>
鳥ラベル・・・ $\{0,0,1\}$
</p>

<p>
とラベルの値にセットします。
<br>
その様なラベルが全部で L 組あるので、ラベル全体は図 2 の様に L x M 行列の<a href="../text01/page04.html#const">定数 OP ノード</a>で表す事が出来ます。
</p>

<div class="info">
<input type="checkbox"><b>図2: ラベル:</b>

<p>
L x M 行列のラベルを<a href="../text01/page04.html#const">定数 OP ノード</a>として表現する
<br>
$i$ 行目 の $[y_{i1}, y_{i2}, \cdots, y_{iM}]$ が $i$ 番目の教師信号に対するラベルとなる
</p>

<img src="./img/page04-fig2.png" alt="">
</div>

<br>
<h3>
4. クロスエントロピー
</h3>

<p>
教師信号とラベルの準備が出来たので、次は教師信号をニューラルネットワークに入れて出てきた出力信号とラベルの値が「そっくり」になるように重みとバイアスをディープラーニングにより更新したいと思います。
</p>

<p>
ただし何らかの指標が無いと「そっくり度」がわかりませんので、まずその指標を決めなければいけません。
<br>
この指標は色々ありますが今回は「<b>クロスエントロピー</b>」を利用したいと思います。
</p>

<p>
まず $i$ 番目の教師信号  $\{x_{i1}, x_{i2}, \cdots, x_{iN}\}$ をニューラルネットワークに入れて出てきた出力信号をベクトル $\{y'_{i1}, y'_{i2}, \cdots, y'_{iM}\}$ とします。
<br>
ここで今回は出力層のパーセプトロンの活性化関数を<a href="../text01/page04.html#softmax">SoftMax 関数</a>としたので、
</p>

<p>
\[
0 \leq y'_{ij}\leq 1 \ ,\  \sum_{j=1}^M y'_{ij} = 1
\]
</p>

<p>
という関係が成り立ちます。つまり
</p>

<p>
<b>
「$y'_{ij}$ は $i$ 番目の教師信号がクラス $j$ に属する確率」
</b>
</p>

<p>
を表します。
</p>

<p>
さて $i$ 番目の教師信号に割り振ったラベル $\{y_{i1}, y_{i2}, \cdots, y_{iM}\}$ を使うと、クロスエントロピーは次のように定義されます。
</p>

<div class="info">
<input type="checkbox"><b>定義: クロスエントロピー:</b>


<p>
\[
H(y,y') = -\sum_{i=1}^L \sum_{j=1}^M y_{ij}\log y'_{ij}
\]
</p>

</div>

<p>
このクロスエントロピー $H(y,y')$ は
</p>

<ol>
<li><b>出力とラベルが似てない → $H(y,y')$ の値は大</b></li>
<li><b>出力とラベルが似ている → $H(y,y')$ の値は小</b></li>
</ol>

<p>
というとても良い性質を持っていますので、$H(y,y')$ が可能な限り小さくなる様に重みとバイアスの値を更新すれば良い事が分かります。
</p>

<br>
<h3>
5. クロスエントロピーのデータフローグラフ
</h3>

<p>
ではこのクロスエントロピーをデータフローグラフ化します。
</p>

<p>
まず図 1 に示した L x N 行列の教師信号をニューラルネットワークへ入力すると、ニューラルネットワークの出力信号もやはり図 3 のような L X M 行列となります。
</p>

<div class="info">
<input type="checkbox"><b>図3: 教師信号を入力した時のニューラルネットワークからの出力信号:</b>

<p>
$i$ 行目の $[y'_{i1}, y'_{i2}, \cdots, y'_{iN}]$ が $i$ 番目の教師信号に対応する出力(result)
</p>

<img src="./img/page04-fig3.png" alt="">
</div>

<p>
従ってクロスエントロピーの<a href="../text01/page05.html">自作 OP ノード</a>は<a href="../text01/page04.html#redsum">総和(Reduce_Sum) OP ノード</a>を使って次のように表す事が出来ます。
</p>

<div class="info">
<input type="checkbox"><b>図4: クロスエントロピー OP ノード:</b>

<p>
名前: cross_entropy
<br>
動作: 行列 r と l を受け取り、r に<a href="../text01/page04.html#log">log</a> を通した行列と l を<a href="../text01/page04.html#mul">掛け</a>、さらに<a href="../text01/page04.html#redsum">総和</a>を取って -1 倍して出力する。
</p>

<img src="./img/page04-fig4.png" alt="">
</div>


<br>
<h3>
6. ディープラーニングの実行
</h3>

<p>
では全ての準備が整ったのでいよいよディープラーニングを実行して重みとバイアスを更新します。
<br>
ディープラーニングの方式には色々あるのですが今回は「<b>勾配降下法</b>」を用います。
</p>

<p>
この勾配降下法は非常に難しい数学の知識が必要なのですが、TensorFlow では OP ノードとして既に用意されているので誰でも簡単に利用できます。

<div class="info">
<input type="checkbox"><b>勾配降下法 OP ノード: </b>

<p>
書式:  ノード名 = tf.train.GradientDescentOptimizer( r ).minimize( c )
</p>

<p>
入力テンソル: r は学習率(スカラー)、c はクロスエントロピー
<br>
出力テンソル: 無し
</p>
</div>

<p>
学習率は学習を行う精度と速度を表しています。
<br>
学習率の値が大きいほどニューラルネットワークは適当に学習しますが速く学習が進みます。
<br>
逆に値が小さいとニューラルネットワークはきちんと学習しますが遅く学習が進みます。
<br>
ただ学習率をいくらにしようとも 1 回では学習は終わりませんので、クロスエントロピーが十分小さくなるまで何回も学習を繰り返す必要があります。
</p>

<p>
では最後にディープラーニング全体のデータフローグラフを示してこのページの話を締めたいと思います。
</p>

<div class="info">
<input type="checkbox"><b>図5: ディープラーニング全体のデータフローグラフ:</b>

<img src="./img/page04-fig5.png" alt="">
</div>

<script>PreNext(4,5)</script>
</body>
</html>
