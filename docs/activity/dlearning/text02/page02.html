<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="stylesheet" href="../../../common.css">
<script src="../../../common.js"></script>
<script src="../../../mathjax.js"></script>
<title>2. ニューラルネットワーク</title>
</head>
<body>

<nav class="brcr">
<ol>
<li><a href="../">アクティビティ: TensorFlow によるディープラーニング</a></li>
<li>学習項目: [2] ディープラーニングの基本</li>
<li><script>GetTitle()</script></li>
</ol>
</nav>

<h2><script>GetTitle()</script></h2>

<h3>
1. パーセプトロン
</h3>

<p>
動物の脳は多数(人間の大脳の場合140億個位)の「<b>神経細胞</b>」という細胞がシナプス結合されて出来ています。
<br>
そこでまず数学者はこの「神経細胞」を数学的にモデル化した「<b>パーセプトロン</b>」を考案しました。
</p>

<p>
なお文献によってはパーセプトロンの事を「<b>ユニット</b>」や「<b>ノード</b>」と呼ぶこともあります。
</p>

<p>
さてパーセプトロンは $n$ 個の入力信号に対し 1 個の信号を出力する関数 $f()$ であり、以下の数式で表すことができます。
</P>

<p>
この $f()$ の事を「<a href="https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0">活性化関数</a>」と呼びます。
</p>

<div class="info">
<input type="checkbox"><b>定義: パーセプトロンの式:</b>

\begin{align*}
y &= f \left ( \sum_{i=0}^{n-1} \{w_i\cdot x_i\} + b  \right )  \\
&= f \left ( w_0\cdot x_0 + \cdots + w_{(n-1)}\cdot x_{(n-1)} + b  \right ) 
\end{align*}

<p>
$y$ : 出力信号
</p>
<p>
$f()$ : 活性化関数
</p>
<p>
$x_i$ : $i$ 番目の入力信号、 $(i=0,1,\cdots,n-1)$
</p>
<p>
$w_i$ : 変数、$i$ 番目の入力信号に掛けられる<b>重み</b>(weight)と呼ぶ
</p>
<p>
$b$ : 変数、<b>バイアス</b>(bias)と呼ぶ
</p>

</div>

<p>
活性化関数 $f()$ の選び方には色々な流儀があるのですが、今回はディープラーニングで良く使われている「<a href="https://en.wikipedia.org/wiki/Sigmoid_function">シグモイド関数</a>」と「<a href="https://en.wikipedia.org/wiki/Softmax_function">SoftMax関数</a>」を用いたいと思います。
</p>

<p>
なおパーセプトロンは以下の様なグラフで表すことが出来ます。
<br>
ただしこのグラフは TensorFlow の<a href="../text01/page03.html">データフロー・グラフ</a>とは異なる一般的なグラフです。
<br>
つまりこの図の○はオペレーションではなくて活性化関数 $f()$ を表しています。
</p>

<div class="info">
<input type="checkbox"><b>図1. パーセプトロンのグラフ</b>

<p>
※ <a href="../text01/page03.html">データフロー・グラフ</a>ではなくて一般的なグラフ表現です
</p>

<img src="./img/page02-fig1.png" alt="">
</div>


<br>
<h3>
2. ニューラルネットワーク
</h3>

<p>
次に数学者は「パーセプトロン」を多数結合して「<b>ニューラルネットワーク</b>」を作りました。
</p>

<p>
ニューラルネットワークは「<b>入力層</b>」、「<b>隠れ層</b>(中間層とも言う)」、「<b>出力層</b>」ごとに層(レイヤー)分けされた多層構造になっています。
</p>

<p>
なお各層は 2 層以上の多層構造になったり、再帰構造になったりすることもありますが、今回は話を簡単にするため、特に断りが無い限りネットワークに再帰構造を含まない「<b>フィードフォワードニューラルネットワーク</b>(Feed Forward Neural Network: FFNN)」だけを考えることにします(図 2)。
</p>


<div class="info">
<input type="checkbox"><b>図2. フィードフォワードニューラルネットワーク(Feed Forward Neural Network: FFNN)のグラフ</b>


<p>
※ <a href="../text01/page03.html">データフロー・グラフ</a>ではなくて一般的なグラフ表現です
</p>

<img src="./img/page02-fig2.png" alt="">
</div>

<p>
ただしあまり隠れ層の層数を増やすとディープラーニングに必要な時間が膨大になりますので、今回は中間層が 1 層だけで出来ている 3 層ニューラルネットワークを扱いたいと思います(図3)。
</p>

<p>
※ 正確には「3 層フィードフォワードニューラルネットワーク」ですが、長いので単に「3層ニューラルネットワーク」とだけ書きます
</p>


<div class="info">
<input type="checkbox"><b>図3. 3層ニューラルネットワークのグラフ</b>

<p>
※ <a href="../text01/page03.html">データフロー・グラフ</a>ではなくて一般的なグラフ表現です
</p>


<p>
入力層が N 個、隠れ層が K 個、出力層が M 個のパーセプトロンで出来ているフィードフォワードニューラルネットワーク
</p>

<img src="./img/page02-fig3.png" alt="">

<p>
$w_{ij}^{\textrm h}$ ・・・ 入力層のパーセプトロン No.$i$ の出力信号から、隠れ層のパーセプトロン No.$j$ への入力に掛けられる重み
</p>
<p>
$b_{j}^{\textrm h}$  ・・・ 隠れ層のパーセプトロン No.$j$ のバイアス
</p>

<p>
および
</p>

<p>
$w_{ij}^{\textrm o}$ ・・・ 隠れ層のパーセプトロン No.$i$ の出力信号から、出力層のパーセプトロン No.$j$ への入力に掛けられる重み
</p>
<p>
$b_{j}^{\textrm o}$  ・・・ 出力層のパーセプトロン No.$j$ のバイアス
</p>

</div>

<br>

<script>PreNext(2,5)</script>
</body>
</html>
