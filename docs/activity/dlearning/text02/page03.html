<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="stylesheet" href="../../../common.css">
<script src="../../../common.js"></script>
<script src="../../../mathjax.js"></script>
<title>3. 3層ニューラルネットワークのデータフローグラフ化</title>
</head>
<body>

<nav class="brcr">
<ol>
<li><a href="../">アクティビティ: TensorFlow によるディープラーニング</a></li>
<li>学習項目: [2] ディープラーニングの基本</li>
<li><script>GetTitle()</script></li>
</ol>
</nav>

<h2><script>GetTitle()</script></h2>

<p>
ここでは 3 層ニューラルネットワークのグラフを TensorFlow のデータフローグラフに変換してみましょう。
</p>

<br>
<h3>
1. 入力層
</h3>

<p>
まず入力層のグラフを切り出して図1に示します。
</p>

<div class="info">
<input type="checkbox"><b>図1: 入力層のグラフ:</b>

<img src="./img/page03-fig1.png" alt="">
</div>

<p>
入力層は N 個のパーセプトロンで出来ていて、N 個の信号が入力されます。
<br>
ただ信号は結局のところただの数値の羅列ですので、入力信号は 1 x N 行列の<a href="../text01/page04.html#const">定数 OP ノード</a>(※)として表されます(図2)。
</p>

<p>
※ 「N 次元ベクトル」や「N x 1 行列」でない理由は次のページで取り上げるディープラーニングとの兼ね合いのためです。
</p>

<div class="info">
<input type="checkbox"><b>図2: 入力信号:</b>

<p>
入力信号は 1 x N 行列の<a href="../text01/page04.html#const">定数 OP ノード</a>として表される。
</p>

<img src="./img/page03-fig2.png" alt="">
</div>

<p>
次に入力層をデータフローグラフ化します。色々なやり方があるのですが、一般的に入力層のパーセプトロンは入力されたデータをそのままスルーして出力するだけのものが多いです。
<br>
よって入力層は図 3 のような<a href="../text01/page05.html">自作 OP ノード</a>として表現できます。
</p>

<div class="info">
<input type="checkbox"><b>図3: 入力層 OP ノード:</b>

<p>
名前: input_layer
<br>
動作: 行列 x を受け取ってそのまま出力
</p>

<img src="./img/page03-fig3.png" alt="">
</div>

<br>
<h3>
2. 隠れ層
</h3>

<p>
次に隠れ層のグラフを切り出して図4に示します。
</p>

<div class="info">
<input type="checkbox"><b>図4: 隠れ層のグラフ:</b>


<p>
$w_{ij}^{\textrm h}$ ・・・ 入力層のパーセプトロン No.$i$ の出力信号から、隠れ層のパーセプトロン No.$j$ への入力に掛けられる重み
</p>
<p>
$b_{j}^{\textrm h}$  ・・・ 隠れ層のパーセプトロン No.$j$ のバイアス
</p>

<img src="./img/page03-fig4.png" alt="">
</div>

<p>
隠れ層は K 個のパーセプトロンで出来ていて、それぞれのパーセプトロンは入力層からの出力信号を受取ります。
その際に重みが入力に掛けられ、更にバイアスが足されます。そして最後に活性化関数 $f()$ に通して出力されます。
</p>

<p>
$x_i$ を入力層のパーセプトロン No.$i$ からの出力信号とし、活性化関数 $f()$ を全てのパーセプトロンで共通とすると、隠れ層のパーセプトロン No.$j$ の出力 $y_j^{\textrm h}$ は以下の式で表されます。
</p>

<p>
\[
y_j^{\textrm h} = f \left ( \sum_{i=1}^{N} \{w_{ij}^{\textrm h}\cdot x_i\} + b_j^{\textrm h}  \right ) \ , \ (j=1,2,\cdots,K)
\]
</p>

<p>
さて入力層のパーセプトロンは N 個、隠れ層のパーセプトロンは K 個ありますので、それらの出力を行列で表すと次のようになります。
</p>

<p>
入力層の出力:
\[
X = [x_1, x_2, \cdots, x_N]
\]
</p>

<p>
隠れ層の出力:
\[
Y^{\textrm h} = [y_1^{\textrm h}, y_2^{\textrm h}, \cdots, y_K^{\textrm h}]
\]
</p>

また重み $w_{ij}^{\textrm h}$ は N x K 行列、バイアス $b_{j}^{\textrm h}$ は 1 x K 行列で表すことができます。

<p>
重み:
\[
W^{\textrm h} = 
\left [
\begin{array}{ccc}
w_{11}^{\textrm h} & \cdots & w_{1K}^{\textrm h} \\
\vdots & \ddots & \vdots \\
w_{N1}^{\textrm h} & \cdots & w_{NK}^{\textrm h}
\end{array}
\right ]
\]
</p>

<p>
バイアス:
\[
B^{\textrm h} = 
[b_1^{\textrm h}, b_2^{\textrm h}, \cdots, b_K^{\textrm h}]
\]
</p>

<p>
以上の行列を使うと、隠れ層の出力 $Y^{\textrm h}$ は以下の行列演算で求められます。なお $X\cdot W^{\textrm h}$ は $X$ と $W^{\textrm h}$ の行列積、$f()$ は行列の全て要素に対して活性化関数の演算を行う事を意味します。
</p>

<p>
\[
Y = f \left ( X\cdot W^{\textrm h} + B^{\textrm h} \right )
\]
</p>

<p>
ではこの行列演算を元に隠れ層をデータフローグラフ化してみましょう。
</p>

<p>
まず重みとバイアスは前のページで触れたように定数ではなくてディープラーニングによって値が変化する変数ですので<a href="../text01/page04.html#var">変数 OP ノード</a>で表現します。
<br>
また要素は一般的には乱数を使って初期化します。
</p>


<div class="info">
<input type="checkbox"><b>図5: 隠れ層の重みとバイアス:</b>

<p>
重み $W^{\textrm h}$ を N x K 行列の<a href="../text01/page04.html#var">変数 OP ノード</a>、バイアス $B^{\textrm h}$ を 1 x K 行列の<a href="../text01/page04.html#var">変数 OP ノード</a>で表す。
<br>
各要素は乱数で初期化する。
</p>

<img src="./img/page03-fig5.png" alt="">
</div>

<p>
次に隠れ層をデータフローグラフ化します。活性化関数 $f()$ の選び方は色々な流儀がありますが、今回のアクティビティでは「<a href="../text01/page04.html#log">シグモイド(sigmoid)関数</a>」を使いたいと思います。
<br>
すると隠れ層は図 6 のような<a href="../text01/page05.html">自作 OP ノード</a>として表現できます。
</p>

<div class="info">
<input type="checkbox"><b>図6: 隠れ層 OP ノード:</b>

<p>
名前: hidden_layer
<br>
動作: 行列 x、w、b を受け取り、行列積 x・w に b を足してシグモイド関数に通して出力する。
</p>

<img src="./img/page03-fig6.png" alt="">
</div>

<h3>
3. 出力層
</h3>

<p>
最後に出力層のグラフを図7に切り出して示します。
</p>

<div class="info">
<input type="checkbox"><b>図7: 出力層のグラフ:</b>

<p>
$w_{ij}^{\textrm o}$ ・・・ 隠れ層のパーセプトロン No.$i$ の出力信号から、出力層のパーセプトロン No.$j$ への入力に掛けられる重み
</p>
<p>
$b_{j}^{\textrm o}$  ・・・ 出力層のパーセプトロン No.$j$ のバイアス
</p>

<img src="./img/page03-fig7.png" alt="">
</div>

<p>
出力層は M 個のパーセプトロンで出来ていて、それぞれのパーセプトロンは隠れ層からの出力信号を受取ります。
<br>
その際に重みが入力に掛けられ、更にバイアスが足されます。
<br>
そして最後に活性化関数 $f()$ に通して出力されます。
</p>

<p>
つまり、隠れ層と全く同じ構造をしているので、隠れ層と同じ様なデータフローグラフとなります。
</p>

<p>
という訳で、出力層の重みとバイアスも隠れ層と同様に<a href="../text01/page04.html#var">変数 OP ノード</a>で表現します。
</p>


<div class="info">
<input type="checkbox"><b>図8: 出力層の重みとバイアスのデータフローグラフ:</b>

<p>
重み $W^{\textrm o}$を K x M 行列の<a href="../text01/page04.html#var">変数 OP ノード</a>、バイアス $B^{\textrm o}$を 1 x M 行列の<a href="../text01/page04.html#var">変数 OP ノード</a>として表現する。
<br>
各要素は乱数で初期化する。
</p>

<img src="./img/page03-fig8.png" alt="">
</div>

<p>
出力層の<a href="../text01/page05.html">自作 OP ノード</a>も図 9 のように表現できます。
<br>
やはり出力層の活性化関数 $f()$ の選び方も色々な流儀がありますが、今回のアクティビティでは「<a href="../text01/page04.html#softmax">SoftMax 関数</a>」を使いたいと思います。
</p>

<div class="info">
<input type="checkbox"><b>図9: 出力層 OP ノード:</b>

<p>
名前: output_layer
<br>
動作: 行列 x、w、b を受け取り、行列積 x・w に b を足して SoftMax 関数に通して出力する。
</p>

<img src="./img/page03-fig9.png" alt="">
</div>

<br>
<h3>
4. 全体のデータフローグラフ
</h3>

<p>
以上の各ノードを組み合わせると、3層ニューラルネットワーク全体では次のようなデータフローグラフとなります。
</p>

<div class="info">
<input type="checkbox"><b>図10: 3層ニューラルネットワーク全体のデータフローグラフ:</b>

<img src="./img/page03-fig10.png" alt="">
</div>


<br>

<script>PreNext(3,5)</script>
</body>
</html>
