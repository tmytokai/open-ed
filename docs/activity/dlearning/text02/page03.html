<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="stylesheet" href="../../../common.css">
<script src="../../../common.js"></script>
<script src="../../../mathjax.js"></script>
<title>3. 3層ニューラルネットワークのデータフローグラフ化</title>
</head>
<body>

<nav class="brcr">
<ol>
<li><a href="../">アクティビティ: TensorFlow によるディープラーニング</a></li>
<li>学習項目: [2] ディープラーニングの基本</li>
<li><script>GetTitle()</script></li>
</ol>
</nav>

<h2><script>GetTitle()</script></h2>

<p>
ここでは 3 層ニューラルネットワークのグラフを TensorFlow のデータフローグラフに変換してみましょう。
</p>

<h3>
1. 入力層
</h3>

<p>
まず入力層のグラフを切り出して図1に示します。
</p>

<div class="info">
<input type="checkbox"><b>図1: 入力層のグラフ:</b>

<p>
</p>

<img src="./img/page03-fig1.png" alt="">
</div>

<p>
入力層は N 個のパーセプトロンで出来ていて、N 個のデータが入力されます。
そこでまずは入力データから TensorFlow のデータフローグラフ化してみます。
</p>

<p>
ディジタル画像のピクセル値とか、ディジタル音声の振幅とか、センサー出力値とかは結局のところただの数値データの羅列ですので、これらは N 次元ベクトルとして表すことが出来ます。そこで 「1 x N 行列の定数」(※)として入力データをデータフロー化します(図2)。
</p>

<p>
※ 「N 次元ベクトル」や「N x 1 行列」でない理由は次のページで取り上げるディープラーニングとの兼ね合いのためです。
</p>

<div class="info">
<input type="checkbox"><b>図2: 入力データのデータフローグラフ:</b>

<p>
1 x N 行列の入力データを定数 OP ノード として表現する
</p>

<img src="./img/page03-fig2.png" alt="">
</div>

<p>
次に入力層をデータフローグラフ化します。色々なやり方があるのですが、一般的に入力層のパーセプトロンは入力されたデータをそのままスルーして出力するだけのものが多いです。よって入力層は図 3 のような自作 OP ノードとして表現できます。
</p>

<div class="info">
<input type="checkbox"><b>図3: 入力層 OP ノード内部のデータフローグラフ:</b>

<p>
名前: input_layer
<br>
動作: 1 x N 行列 の x を受け取ってそのまま出力する。
</p>

<img src="./img/page03-fig3.png" alt="">
</div>

<h3>
2. 隠れ層
</h3>

<p>
次に隠れ層のグラフを切り出して図4に示します。
<p>

<div class="info">
<input type="checkbox"><b>図4: 隠れ層のグラフ:</b>

<p>
$w_{ij}$ ・・・ 入力層のパーセプトロン No.$i$ の出力から、隠れ層のパーセプトロン No.$j$ への入力に掛けられる重み
<br>
$b_{j}$ ・・・ 隠れ層のパーセプトロン No.$j$ のバイアス
</p>

<img src="./img/page03-fig4.png" alt="">
</div>

<p>
隠れ層は K 個のパーセプトロンで出来ていて、それぞれのパーセプトロンは入力層からの出力を受取ります。
その際に重みが入力に掛けられ、更にバイアスが足されます。そして最後に活性化関数 $f()$ に通して出力されます。
</p>

<p>
$x_i$ を入力層のパーセプトロン No.$i$ からの出力とし、活性化関数 $f()$ を全てのパーセプトロンで共通とすると、隠れ層のパーセプトロン No.$j$ の出力 $y_j$ は以下の式で表されます。
</p>

<p>
\[
y_j = f \left ( \sum_{i=1}^{N} \{w_{ij}x_i\} + b_j  \right ) 
\]
</p>

<p>
さて入力層のパーセプトロンは N 個、隠れ層のパーセプトロンは K 個ありますので、それらの出力を行列で表すと次のようになります。
</p>

<p>
入力層の出力:
\[
X = [x_1, x_2, \cdots, x_N]
\]
</p>

<p>
隠れ層の出力:
\[
Y = [y_1, y_2, \cdots, y_K]
\]
</p>

また重み $w_{ij}$ は N x K 行列、バイアス $b_{j}$ は 1 x K 行列で表すことができます。

<p>
重み:
\[
W = 
\left [
\begin{array}{ccc}
w_{11} & \cdots & w_{1K} \\
\vdots & \ddots & \vdots \\
w_{N1} & \cdots & w_{NK}
\end{array}
\right ]
\]
</p>

<p>
バイアス:
\[
B = 
[b_1, b_2, \cdots, b_K]
\]
</p>

<p>
以上の行列を使うと、隠れ層の出力 $Y$ は以下の行列演算で求められます。なお $XW$ は $X$ と $W$ の行列積、$f()$ は行列の全て要素に対して活性化関数の演算を行う事を意味します。
</p>

<p>
\[
Y = f \left ( XW + B \right )
\]
</p>

<p>
ではこの行列演算を元に隠れ層をデータフローグラフ化してみましょう。
</p>

<p>
まず重みとバイアスは前のページで触れたように定数ではなくてディープラーニングによって値が変化する変数ですので変数 OP ノードで表現します。なお要素は一般的には乱数を使って初期化します。
</p>


<div class="info">
<input type="checkbox"><b>図5: 隠れ層の重みとバイアスのデータフローグラフ:</b>

<p>
重みを N x K 行列の変数 OP ノード、バイアスを 1 x K 行列の変数 OP ノードとして表現する。
<br>
各要素は乱数で初期化する。
</p>

<img src="./img/page03-fig5.png" alt="">
</div>

<p>
次に隠れ層をデータフローグラフ化します。活性化関数 $f()$ の選び方は色々な流儀がありますが、今回のアクティビティでは「シグモイド(sigmoid)関数」を使いたいと思います。
すると隠れ層は図 6 のような自作 OP ノードとして表現できます。
</p>

<div class="info">
<input type="checkbox"><b>図6: 隠れ層 OP ノード内部のデータフローグラフ:</b>

<p>
名前: hidden_layer
<br>
動作: 1 x N 行列 の入力層の出力 x、N x K 行列 の重み w、1 x K 行列 のバイアス b を受け取ってシグモイド(sigmoid)関数に通して出力する。
</p>

<p>
※行列積の順番は x・w 
</p>

<img src="./img/page03-fig6.png" alt="">
</div>

<h3>
3. 出力層
</h3>

<p>
最後に出力層のグラフを図7に切り出して示します。
<p>

<div class="info">
<input type="checkbox"><b>図7: 出力層のグラフ:</b>

<p>
$w_{ij}$ ・・・ 隠れ層のパーセプトロン No.$i$ の出力から、出力層のパーセプトロン No.$j$ への入力に掛けられる重み
<br>
$b_{j}$ ・・・ 出力層のパーセプトロン No.$j$ のバイアス
</p>

<img src="./img/page03-fig7.png" alt="">
</div>

<p>
出力層は M 個のパーセプトロンで出来ていて、それぞれのパーセプトロンは隠れ層からの出力を受取ります。
その際に重みが入力に掛けられ、更にバイアスが足されます。そして最後に活性化関数 $f()$ に通して出力されます。
</p>

<p>
つまり、隠れ層と全く同じ構造をしているので、隠れ層と同じ様なデータフローグラフとなります。
</p>

<p>
という訳で、出力層の重みとバイアスも隠れ層と同様に変数 OP ノードで表現します。
</p>


<div class="info">
<input type="checkbox"><b>図8: 出力層の重みとバイアスのデータフローグラフ:</b>

<p>
重みを K x M 行列の変数 OP ノード、バイアスを 1 x M 行列の変数 OP ノードとして表現する。
<br>
各要素は乱数で初期化する。
</p>

<img src="./img/page03-fig8.png" alt="">
</div>

<p>
出力層の自作 OP ノードも図 9 のように表現できます。
やはり出力層の活性化関数 $f()$ の選び方も色々な流儀がありますが、今回のアクティビティでは「SoftMax 関数」を使いたいと思います。
</p>

<div class="info">
<input type="checkbox"><b>図9: 出力層 OP ノード内部のデータフローグラフ:</b>

<p>
名前: output_layer
<br>
動作: 1 x K 行列 の隠れ層の出力 x、K x M 行列 の重み w、1 x M 行列 のバイアス b を受け取って SoftMax 関数に通して出力する。
</p>

<p>
※行列積の順番は x・w 
</p>

<img src="./img/page03-fig9.png" alt="">
</div>

<h3>
4. 全体のデータフローグラフ
</h3>

<p>
以上の様にして作った各層の自作 OP ノードを使うと、3層ニューラルネットワーク全体では次のようなデータフローグラフとなります。
</p>

<div class="info">
<input type="checkbox"><b>図10: 3層ニューラルネットワーク全体のデータフローグラフ:</b>

<img src="./img/page03-fig10.png" alt="">
</div>


<br>

<script>PreNext(3,5)</script>
</body>
</html>
